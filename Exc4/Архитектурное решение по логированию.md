# Логирование

## Анализ

В случае с "Александритом" не будет правильно рассматривать логирование в отрыве от трейсинга. Логирование поможет проанализировать результаты трейсинга, а трейсинг обогатит контекст для анализа логирования. Тем не менее это не значит что результатами логирования нельзя пользоваться в отрыве от трейсинга. Собственный опыт показывает что грамотно составленные логи позволяют отследить и выявить значительную часть проблем

Предполагается что все сообщения будут содержать минимальный полезный контекст (время, id запроса, id клиента, id заказа, etc). Мы будем использовать стандартные 5 уровня для логирования:

- DEBUG - дополнительная информация для улучшения отслеживания, например:
  - вхождение в блокирующую операцию
  - ожидание получения контроля над примитивами синхронизации
  - дампы структур данных
  - дампы сетевых запросов
- INFO - основной уровень логирования, если система поставляется внешним пользователям, то это минимальный уровень логов, которые он увидит, соответственно они должны быть достаточны для базовой диагностики ситуации, при этом оставаясь достаточно краткими, чтобы не создавать дополнительную когнитивную нагрузку на персонал. В случае с "Александритом" здесь можно будет найти:
  - начало обработки в точке входа в сервис
  - изменение статуса заказа
  - запросы с другие сервисы
  - окончание обработки запроса в рамках текущего сервиса
- WARNING - отклонения от нормальной работы и не только, например:
  - долгое время обработки
  - несовпадение версий
  - депрекация функциональности и интерфейсов
- ERROR - нефатальные для работы системы ошибки, например:
  - ошибки бизнес логики
  - ошибки работы сервисов
  - ошибки маршрутизации
- CRITICAL - ошибка, которая приведёт к полной неработоспособности системы или просто полная неработоспобность сервиса (a.k.a зовите SRE), например:
  - ошибка сегментации
  - исчерпание важного ресурса (например встали все потоки обработки)
  - сервис уже 10 минут не отвечает на healthcheck

Естественно это более чем примерный список, поскольку предсказать необходимые логи на этапе проектирования системы не всегда представляется возможным, на это окажет большое влияние конечный дизайн сервиса, который часто оказывается на откупе у исполнителя местного уровня

## Мотивация

Логирование - это практически первый ресурс к которому обращаются разработчики для диагностики и устранения проблем связанных с работой ПО. Один из самых глупых вопросов заданных мне как разработчику был: "А кому из заказчиков нужно логирование?". Логирование незаменимо для любой системы, если нужно выбирать из троицы наблюдаемости что-то одно, то я буду выступать именно за логирование. Оно помогает решить множество задач, стоящих перед техническими специалистами:

- диагностика и устранение ошибок
- мониторинг системы (естественно что гораздо хуже обычного)
- аудит безопасности - логирование действий пользователя в контексте сервисов
- упрощение поддержки - логи представляют ценный ресурс для анализа командой поддержки и devops инженерами
- повышение надёжности - чем лучше логи, тем проще выявлять ошибки и аномалии, тем самым снижая их общее кол-во
- автоматизация анализа - хороший и правильно структурированный лог можно скормить нейронке с целью автоматизации анализа поведения системы

Если выбирать какие системы стоит в первую очередь покрыть логированием и трейсингом - то это именно API компоненты, если максимально сузить, то можно уменьшить список всего до двух элементов:

- Shop API
- MES API

Эти два компонента скорее всего окажутся наиболее полезны для решения проблем с которыми столкнулся "Александрит". Точки входа запросов будут наиболее подходящими точками для первичного анализа

## Предлагаемое решение

Для первоначального хранения логов предлагаю воспользоваться стандартными средствами для современных Linux систем, а именно journald из состава systemd. Это обеспечит локальный доступ для разработчиков (позволяя работать над элементами всей системы в отрыве от основной инфраструктуры) и буферную зону с случае недоступности сервиса центрального логирования. Соответственно необходимости долговременного хранения на источниках логов отсутствует, 1 день будет более чем достаточен. Контроль доступа будет осуществляться стандартными средствами, для просмотра лога по умолчанию и так требуются повышенные привилегии

Выбор технологий для обеспечения логирования в основном обоснован применением grafana как центрального интерфейса для взаимодействия с мониторингом и трейсингом. ELK стек тоже бы прекрасно справился. Итого:

- promtail для экспорта данных логирования их системного журнала
- loki для обработки логов
- grafana для отображения, фильтрации, анализа

Хранение в loki разделено на 2 типа:

- хранение индексов - для этого мы будем использовать TSDB
- хранения chunk данных - для этого мы будем использовать S3 совместимый MinIO

Размер хранилищ тяжело предсказать, не имея данных по объёму файлов. Но чисто эмпирически их должно хватить для того чтобы хранить данные за неделю, это должно дать возможность команде как и заметить проблему так и успеть её проанализировать

Существует как минимум два аспекта безопасности, которые стоит учитывать. Один со стороны инфраструктуры, а второй со стороны ACL

Требования к инфраструктуре:

- зашифрованные каналы передачи данных
- фиксированные endpoint для данных
- маршрутизация трафика, позволяющая данных логирования попадать только на сам бекенд логирования, а оттуда в БД или на grafana

Со стороны общих правил безопасности:

- доступ к мониторингу (а соответственно и к логированию) только из intranet
- ограниченный доступ к intranet извне (только защищенные соединения с логирование, двух-факторная аутентификация)
- ролевая модель авторизации

## Платформа для анализа логов

Если мы договоримся что основной алёртинг работает на основе данных мониторинга, логирование, тем не менее может предоставить нам несколько полезных алёртов:

- события уровня critical. Это важные события, которые могут не сразу отразится на мониторинге (или вообще не отразится, если сервис имеет автоматическую перезагрузку например)
- попытки несанкционированного доступа, логи от antifraud систем, подбор паролей и т.д. Некоторые логи активности пользователя можно использовать для алёртинга подозрительных ситуаций

Логи могут использоваться для поиска аномалий, в том числе автоматизированного (в том числе и с помощью инструментов искусственного интеллекта), но я считаю пример приведённый в задании таковым не является. DDoS атака тривиальна видна на мониторинге (хотя не спорю, та гора логов что сгенерирует DDoS атака - хороший индикатор того что она происходит). Логи это хороший, но обычно не оперативный инструмент анализа, в первую очередь от них нам нужен исторический анализ
